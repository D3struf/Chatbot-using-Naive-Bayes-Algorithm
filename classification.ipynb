{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower() #Convert the sentences into lowercase\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #Tokenize on word charcter\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')] # remove stopwords\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Big brown fox jumped over a lazy dog.\"\n",
    "sentence2 = \"This is particularly important in today's world where we are swamped with unstructured natural language data on the variety of social media platforms people engage in now-a-days (note -  now-a-days in the decade of 2010-2020)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sentence = preprocess(sentence)\n",
    "print(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['particularly',\n",
       " 'important',\n",
       " 'today',\n",
       " 'world',\n",
       " 'swamped',\n",
       " 'unstructured',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'variety',\n",
       " 'social',\n",
       " 'media',\n",
       " 'platforms',\n",
       " 'people',\n",
       " 'engage',\n",
       " 'days',\n",
       " 'note',\n",
       " 'days',\n",
       " 'decade',\n",
       " '2010',\n",
       " '2020']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(preprocessed_sentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particularly', 'RB'), ('important', 'JJ'), ('today', 'NN'), ('world', 'NN'), ('swamped', 'VBD'), ('unstructured', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('variety', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('people', 'NNS'), ('engage', 'VBP'), ('days', 'NNS'), ('note', 'VBP'), ('days', 'NNS'), ('decade', 'NN'), ('2010', 'CD'), ('2020', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "tags2 = nltk.pos_tag(preprocess(sentence2))\n",
    "print(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tagged(sentences):\n",
    "    features = []\n",
    "    for tagged_word in sentences:\n",
    "        word, tag = tagged_word\n",
    "        if tag=='NN' or tag == 'VBN' or tag == 'NNS' or tag == 'VBP' or tag == 'RB' or tag == 'VBZ' or tag == 'VBG' or tag =='PRP' or tag == 'JJ':\n",
    "            features.append(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['particularly',\n",
       " 'important',\n",
       " 'today',\n",
       " 'world',\n",
       " 'unstructured',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'variety',\n",
       " 'social',\n",
       " 'media',\n",
       " 'platforms',\n",
       " 'people',\n",
       " 'engage',\n",
       " 'days',\n",
       " 'note',\n",
       " 'days',\n",
       " 'decade']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tagged(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def extract_feature(text):\n",
    "    words = preprocess(text)\n",
    "#     print('words: ',words)\n",
    "    tags = nltk.pos_tag(words)\n",
    "#     print('tags: ',tags)\n",
    "    extracted_features = extract_tagged(tags)\n",
    "#     print('Extracted features: ',extracted_features)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extracted_features]\n",
    "#     print(stemmed_words)\n",
    "    result = [lmtzr.lemmatize(x) for x in stemmed_words]\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Big brown fox jumped over a lazy dog.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "words = extract_feature(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['particular', 'import', 'today', 'world', 'unstructur', 'natur', 'languag', 'data', 'varieti', 'social', 'medium', 'platform', 'peopl', 'engag', 'day', 'note', 'day', 'decad']\n"
     ]
    }
   ],
   "source": [
    "words = extract_feature(sentence2)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hurt', 'right', 'foot', 'wear', 'white', 'shoe', 'foot']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature(\"He hurt his right foot while he was wearing white shoes on his feet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'particular': True,\n",
       " 'import': True,\n",
       " 'today': True,\n",
       " 'world': True,\n",
       " 'unstructur': True,\n",
       " 'natur': True,\n",
       " 'languag': True,\n",
       " 'data': True,\n",
       " 'varieti': True,\n",
       " 'social': True,\n",
       " 'medium': True,\n",
       " 'platform': True,\n",
       " 'peopl': True,\n",
       " 'engag': True,\n",
       " 'day': True,\n",
       " 'note': True,\n",
       " 'decad': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feats(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    corpus = []\n",
    "    # The responses of the chat bot\n",
    "    answers = {}\n",
    "    for (text,category,answer) in data:\n",
    "\n",
    "        features = extract_feature(text)\n",
    "\n",
    "        corpus.append(features)\n",
    "        result.append((word_feats(features), category))\n",
    "        answers[category] = answer\n",
    "\n",
    "    return (result, sum(corpus,[]), answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([({'input': True, 'user': True}, 'category')],\n",
       " ['input', 'user'],\n",
       " {'category': 'answer to give'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature_from_doc([['this is the input text from the user','category','answer to give']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'], 'responses': ['Hello, thanks for visiting', 'Good to see you again', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later, thanks for visiting', 'Have a nice day', 'Bye! Come back again soon.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'hours', 'patterns': ['What hours are you open?', 'What are your hours?', 'When are you open?'], 'responses': [\"We're open every day 9am-9pm\", 'Our hours are 9am-9pm every day']}, {'tag': 'mopeds', 'patterns': ['Which mopeds do you have?', 'What kinds of mopeds are there?', 'What do you rent?'], 'responses': ['We rent Yamaha, Piaggio and Vespa mopeds', 'We have Piaggio, Vespa and Yamaha mopeds']}, {'tag': 'payments', 'patterns': ['Do you take credit cards?', 'Do you accept Mastercard?', 'Are you cash only?'], 'responses': ['We accept VISA, Mastercard and AMEX', 'We accept most major credit cards']}, {'tag': 'opentoday', 'patterns': ['Are you open today?', 'When do you open today?', 'What are your hours today?'], 'responses': [\"We're open every day from 9am-9pm\", 'Our hours are 9am-9pm every day']}, {'tag': 'rental', 'patterns': ['Can we rent a moped?', \"I'd like to rent a moped\", 'How does this work?'], 'responses': ['Are you looking to rent today or later this week?'], 'context_set': 'rentalday'}, {'tag': 'today', 'patterns': ['today'], 'responses': ['For rentals today please call 1-800-MYMOPED', 'Same-day rentals please call 1-800-MYMOPED'], 'context_filter': 'rentalday'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "# Now, 'intents_data' contains your JSON data as a Python dictionary\n",
    "print(intents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m features_data, corpus, answers \u001b[38;5;241m=\u001b[39m extract_feature_from_doc(intents_data)\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mextract_feature_from_doc\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# The responses of the chat bot\u001b[39;00m\n\u001b[0;32m      5\u001b[0m answers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (text,category,answer) \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m      8\u001b[0m     features \u001b[38;5;241m=\u001b[39m extract_feature(text)\n\u001b[0;32m     10\u001b[0m     corpus\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "features_data, corpus, answers = extract_feature_from_doc(intents_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
