{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower() #Convert the sentences into lowercase\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #Tokenize on word charcter\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')] # remove stopwords\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Big brown fox jumped over a lazy dog.\"\n",
    "sentence2 = \"This is particularly important in today's world where we are swamped with unstructured natural language data on the variety of social media platforms people engage in now-a-days (note -  now-a-days in the decade of 2010-2020)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sentence = preprocess(sentence)\n",
    "print(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['particularly',\n",
       " 'important',\n",
       " 'today',\n",
       " 'world',\n",
       " 'swamped',\n",
       " 'unstructured',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'variety',\n",
       " 'social',\n",
       " 'media',\n",
       " 'platforms',\n",
       " 'people',\n",
       " 'engage',\n",
       " 'days',\n",
       " 'note',\n",
       " 'days',\n",
       " 'decade',\n",
       " '2010',\n",
       " '2020']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(preprocessed_sentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particularly', 'RB'), ('important', 'JJ'), ('today', 'NN'), ('world', 'NN'), ('swamped', 'VBD'), ('unstructured', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('variety', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('people', 'NNS'), ('engage', 'VBP'), ('days', 'NNS'), ('note', 'VBP'), ('days', 'NNS'), ('decade', 'NN'), ('2010', 'CD'), ('2020', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "tags2 = nltk.pos_tag(preprocess(sentence2))\n",
    "print(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tagged(sentences):\n",
    "    features = []\n",
    "    for tagged_word in sentences:\n",
    "        word, tag = tagged_word\n",
    "        if tag=='NN' or tag == 'VBN' or tag == 'NNS' or tag == 'VBP' or tag == 'RB' or tag == 'VBZ' or tag == 'VBG' or tag =='PRP' or tag == 'JJ':\n",
    "            features.append(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['particularly',\n",
       " 'important',\n",
       " 'today',\n",
       " 'world',\n",
       " 'unstructured',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'variety',\n",
       " 'social',\n",
       " 'media',\n",
       " 'platforms',\n",
       " 'people',\n",
       " 'engage',\n",
       " 'days',\n",
       " 'note',\n",
       " 'days',\n",
       " 'decade']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tagged(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def extract_feature(text):\n",
    "    words = preprocess(text)\n",
    "#     print('words: ',words)\n",
    "    tags = nltk.pos_tag(words)\n",
    "#     print('tags: ',tags)\n",
    "    extracted_features = extract_tagged(tags)\n",
    "#     print('Extracted features: ',extracted_features)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extracted_features]\n",
    "#     print(stemmed_words)\n",
    "    result = [lmtzr.lemmatize(x) for x in stemmed_words]\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Big brown fox jumped over a lazy dog.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'brown', 'fox', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "words = extract_feature(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['particular', 'import', 'today', 'world', 'unstructur', 'natur', 'languag', 'data', 'varieti', 'social', 'medium', 'platform', 'peopl', 'engag', 'day', 'note', 'day', 'decad']\n"
     ]
    }
   ],
   "source": [
    "words = extract_feature(sentence2)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hurt', 'right', 'foot', 'wear', 'white', 'shoe', 'foot']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature(\"He hurt his right foot while he was wearing white shoes on his feet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'particular': True,\n",
       " 'import': True,\n",
       " 'today': True,\n",
       " 'world': True,\n",
       " 'unstructur': True,\n",
       " 'natur': True,\n",
       " 'languag': True,\n",
       " 'data': True,\n",
       " 'varieti': True,\n",
       " 'social': True,\n",
       " 'medium': True,\n",
       " 'platform': True,\n",
       " 'peopl': True,\n",
       " 'engag': True,\n",
       " 'day': True,\n",
       " 'note': True,\n",
       " 'decad': True}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feats(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    corpus = []\n",
    "    # The responses of the chat bot\n",
    "    answers = {}\n",
    "    for (text,category,answer) in data:\n",
    "\n",
    "        features = extract_feature(text)\n",
    "\n",
    "        corpus.append(features)\n",
    "        result.append((word_feats(features), category))\n",
    "        answers[category] = answer\n",
    "\n",
    "    return (result, sum(corpus,[]), answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([({'input': True, 'user': True}, 'category')],\n",
       " ['input', 'user'],\n",
       " {'category': 'answer to give'})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature_from_doc([['this is the input text from the user','category','answer to give']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'], 'responses': ['Hello, thanks for visiting', 'Good to see you again', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later, thanks for visiting', 'Have a nice day', 'Bye! Come back again soon.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'hours', 'patterns': ['What hours are you open?', 'What are your hours?', 'When are you open?'], 'responses': [\"We're open every day 9am-9pm\", 'Our hours are 9am-9pm every day']}, {'tag': 'mopeds', 'patterns': ['Which mopeds do you have?', 'What kinds of mopeds are there?', 'What do you rent?'], 'responses': ['We rent Yamaha, Piaggio and Vespa mopeds', 'We have Piaggio, Vespa and Yamaha mopeds']}, {'tag': 'payments', 'patterns': ['Do you take credit cards?', 'Do you accept Mastercard?', 'Are you cash only?'], 'responses': ['We accept VISA, Mastercard and AMEX', 'We accept most major credit cards']}, {'tag': 'opentoday', 'patterns': ['Are you open today?', 'When do you open today?', 'What are your hours today?'], 'responses': [\"We're open every day from 9am-9pm\", 'Our hours are 9am-9pm every day']}, {'tag': 'rental', 'patterns': ['Can we rent a moped?', \"I'd like to rent a moped\", 'How does this work?'], 'responses': ['Are you looking to rent today or later this week?'], 'context_set': 'rentalday'}, {'tag': 'today', 'patterns': ['today'], 'responses': ['For rentals today please call 1-800-MYMOPED', 'Same-day rentals please call 1-800-MYMOPED'], 'context_filter': 'rentalday'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents_data = json.load(file)\n",
    "\n",
    "# Now, 'intents_data' contains your JSON data as a Python dictionary\n",
    "print(intents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data, corpus, answers = extract_feature_from_doc(intents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(features_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What do you rent?', 'mopeds'),\n",
       " (\"That's helpful\", 'thanks'),\n",
       " ('Hi', 'greeting'),\n",
       " ('What are your hours?', 'hours'),\n",
       " ('Are you cash only?', 'payments'),\n",
       " ('See you later', 'goodbye'),\n",
       " ('Good day', 'greeting'),\n",
       " ('Do you take credit cards?', 'payments'),\n",
       " ('How does this work?', 'rental'),\n",
       " ('When are you open?', 'hours'),\n",
       " ('When do you open today?', 'opentoday'),\n",
       " ('Goodbye', 'goodbye'),\n",
       " ('Can we rent a moped?', 'rental'),\n",
       " ('Hello', 'greeting'),\n",
       " ('today', 'today'),\n",
       " ('What hours are you open?', 'hours'),\n",
       " ('What are your hours today?', 'opentoday'),\n",
       " ('Is anyone there?', 'greeting'),\n",
       " ('What kinds of mopeds are there?', 'mopeds'),\n",
       " ('Are you open today?', 'opentoday'),\n",
       " ('Thank you', 'thanks'),\n",
       " ('Which mopeds do you have?', 'mopeds'),\n",
       " ('Thanks', 'thanks'),\n",
       " ('Do you accept Mastercard?', 'payments'),\n",
       " ('How are you', 'greeting'),\n",
       " ('Bye', 'goodbye'),\n",
       " (\"I'd like to rent a moped\", 'rental')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greeting': 'Hi there, how can I help?',\n",
       " 'goodbye': 'Bye! Come back again soon.',\n",
       " 'thanks': 'My pleasure',\n",
       " 'hours': 'Our hours are 9am-9pm every day',\n",
       " 'mopeds': 'We have Piaggio, Vespa and Yamaha mopeds',\n",
       " 'payments': 'We accept most major credit cards',\n",
       " 'opentoday': 'Our hours are 9am-9pm every day',\n",
       " 'rental': 'Are you looking to rent today or later this week?',\n",
       " 'today': 'Same-day rentals please call 1-800-MYMOPED'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test sets\n",
    "split_ratio = 0.8\n",
    "\n",
    "def split_dataset(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    data_length = len(data)\n",
    "    train_split = int(data_length * split_ratio)\n",
    "    return (data[:train_split]), (data[train_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = split_dataset(features_data, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What do you rent?', 'mopeds'),\n",
       " (\"That's helpful\", 'thanks'),\n",
       " ('Hi', 'greeting'),\n",
       " ('What are your hours?', 'hours'),\n",
       " ('Are you cash only?', 'payments')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Which mopeds do you have?', 'mopeds'),\n",
       " ('Thanks', 'thanks'),\n",
       " ('Do you accept Mastercard?', 'payments'),\n",
       " ('How are you', 'greeting'),\n",
       " ('Bye', 'goodbye')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "np.save('training_data', training_data)\n",
    "np.save('test_data', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION USING NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_naive_bayes(training_data, test_data):\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, training_data)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "    return classifier, classifier_name, test_set_accuracy, training_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_features(text):\n",
    "    return dict([(word, True) for word in word_tokenize(text)])\n",
    "\n",
    "# Train using Naive Bayes classifier\n",
    "def train_using_naive_bayes(training_data, test_data):\n",
    "    # Extract features from training data\n",
    "    # training_features = [(extract_features(text), category) for text, category in training_data]\n",
    "    \n",
    "    # Train the Naive Bayes classifier\n",
    "    classifier = NaiveBayesClassifier.train(training_data)\n",
    "    \n",
    "    classifier_name = type(classifier).__name__\n",
    "    \n",
    "    # Evaluate classifier on training set\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, training_data)\n",
    "    \n",
    "    # Extract features from test data\n",
    "    test_features = [(extract_features(text), category) for text, category in test_data]\n",
    "    \n",
    "    # Evaluate classifier on test set\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_features)\n",
    "    \n",
    "    return classifier, classifier_name, training_set_accuracy, test_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the classifier using Naive Bayes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m classifier, classifier_name, training_set_accuracy, test_set_accuracy  \u001b[38;5;241m=\u001b[39m train_using_naive_bayes(features_data, test_data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print accuracies and other information\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_set_accuracy)\n",
      "Cell \u001b[1;32mIn[200], line 13\u001b[0m, in \u001b[0;36mtrain_using_naive_bayes\u001b[1;34m(training_data, test_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_using_naive_bayes\u001b[39m(training_data, test_data):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract features from training data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# training_features = [(extract_features(text), category) for text, category in training_data]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Train the Naive Bayes classifier\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m NaiveBayesClassifier\u001b[38;5;241m.\u001b[39mtrain(training_data)\n\u001b[0;32m     15\u001b[0m     classifier_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(classifier)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Evaluate classifier on training set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\classify\\naivebayes.py:212\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.train\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m featureset, label \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets:\n\u001b[0;32m    211\u001b[0m     label_freqdist[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname, fval \u001b[38;5;129;01min\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;66;03m# Increment freq(fval|label, fname)\u001b[39;00m\n\u001b[0;32m    214\u001b[0m         feature_freqdist[label, fname][fval] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# Record that fname can take the value fval.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Train the classifier using Naive Bayes\n",
    "classifier, classifier_name, training_set_accuracy, test_set_accuracy  = train_using_naive_bayes(features_data, test_data)\n",
    "\n",
    "# Print accuracies and other information\n",
    "print(training_set_accuracy)\n",
    "print(test_set_accuracy)\n",
    "print(len(classifier.most_informative_features()))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(({\"That's\": True, 'helpful': True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'help': True}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feats(extract_feature(\"That's helpful\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greeting'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"That's helpful\"\n",
    "classifier.classify(word_feats(extract_feature(input_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input):\n",
    "    # Extract classification from the input sentence\n",
    "    class_input = classifier.classify(word_feats(extract_feature(input)))\n",
    "    print(class_input)\n",
    "    # Initialize response variable\n",
    "    response = None\n",
    "\n",
    "    # Search for the response associated with the tag\n",
    "    for intent in intents_data['intents']:\n",
    "        if intent['tag'] == class_input:\n",
    "            responses = intent['responses']\n",
    "            response = random.choice(responses)\n",
    "            break\n",
    "\n",
    "    print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\n",
      "Response: Good to see you again\n"
     ]
    }
   ],
   "source": [
    "# Input sentence\n",
    "user_input = \"Bye\"\n",
    "get_response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(({\"Bye\": True, 'helpful': False}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
