{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intents from JSON file\n",
    "with open(\"../intents.json\", 'r') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1942 documents\n",
      "133 classes ['admission_admission/enrollmentprocedure_enrollment_graduate', 'admission_admission/enrollmentprocedure_enrollment_oldstudents', 'admission_admission/enrollmentprocedure_enrollment_scholars', 'admission_admission/enrollmentprocedure_foreign', 'admission_admission/enrollmentprocedure_freshmen', 'admission_contact_information_cafa', 'admission_contact_information_cavite', 'admission_contact_information_cie', 'admission_contact_information_cit', 'admission_contact_information_cla', 'admission_contact_information_coe', 'admission_contact_information_cos', 'admission_contact_information_eteeap', 'admission_contact_information_manila', 'admission_contact_information_office', 'admission_contact_information_taguig', 'admission_contact_information_visayas', 'admission_graduateprograms_admissionprocedure', 'admission_graduateprograms_contacts', 'admission_graduateprograms_officeofgraduateprograms', 'admission_graduateprograms_programsoffering', 'admission_graduateprograms_programsoffering_collegeofarchitectureandfinearts', 'admission_graduateprograms_programsoffering_collegeofengineering', 'admission_graduateprograms_programsoffering_collegeofindustrialeducation', 'admission_graduateprograms_programsoffering_collegeofindustrialtechnology', 'admission_graduateprograms_programsoffering_collegeofliberalarts', 'admission_graduateprograms_programsoffering_collegeofscience', 'admission_office', 'admission_undergraduate_procedure', 'admission_undergraduate_programs_requirements_cafa', 'admission_undergraduate_programs_requirements_cie', 'admission_undergraduate_programs_requirements_cit', 'admission_undergraduate_programs_requirements_cla', 'admission_undergraduate_programs_requirements_coe', 'admission_undergraduate_programs_requirements_cos', 'admission_undergraduate_tuition_fee', 'applicationscholarship_link', 'assessmentoffees_link', 'cafa_department_architecture', 'cafa_department_finearts', 'cafa_department_graphics', 'cafa_goals', 'cafa_history', 'cafa_objectives', 'cafa_undergrad_org', 'canteen', 'cie_department_homeeconomics', 'cie_department_professionalindustrialeducation', 'cie_department_studentteaching', 'cie_department_technicalarts', 'cie_goals', 'cie_history', 'cie_objectives', 'cie_undergrad_org', 'cit_department_basicindustrialtech', 'cit_department_civilengtech', 'cit_department_electricalengtech', 'cit_department_electronicsengtech', 'cit_department_foodtech', 'cit_department_graphicsandprinttech', 'cit_department_mechengtech', 'cit_department_powerplanttech', 'cit_goals', 'cit_history', 'cit_objectives', 'cit_undergrad_org', 'cla_department_entrepreneur', 'cla_department_languages', 'cla_department_physicaleducation', 'cla_department_socialscience', 'cla_goals', 'cla_history', 'cla_undergrad_org', 'class_outlook_for_graduate', 'class_starts', 'coe_department_civil', 'coe_department_electrical', 'coe_department_electronics', 'coe_department_mechanical', 'coe_goals', 'coe_history', 'coe_undergrad_org', 'cos_department_chemistry', 'cos_department_computerstudies', 'cos_department_mathematics', 'cos_department_physics', 'cos_goals', 'cos_history', 'cos_objectives', 'cos_undergrad_org', 'developers', 'expanded_tertiary', 'facilities', 'goodbye', 'greeting', 'happiness', 'hours', 'library', 'location', 'mode_of_classes', 'name', 'officials_academic_affairs', 'officials_admin_finance', 'officials_office_president', 'officials_planningDev_specialConcerns', 'officials_research_extensions', 'officials_top_management', 'osa_link', 'passing_rate', 'principal', 'redirection_cavitecampuslink', 'redirection_manilacampuslink', 'redirection_presidentupdates', 'redirection_serviceslinks', 'redirection_taguigcampuslink', 'redirection_visayascampuslink', 'salutation', 'school_news', 'strengths', 'studenthandbook', 'studentprivileges_link', 'studentscholarship_link', 'task', 'transferees_or_late_enrollees', 'tup_code', 'tup_core_values', 'tup_history', 'tup_hymn', 'tup_mandate', 'tup_mission', 'tup_quality_policy', 'tup_strategic_goals', 'tup_vision']\n",
      "1016 unique lemmatized words ['$', '&', \"'\", \"'and\", \"'d\", \"'m\", \"'s\", '(', ')', ',', '.', '12', '1518', '20', '2006', '2022', '4p', '9f', 'a', 'ability', 'about', 'academic', 'acceptance', 'accepted', 'access', 'accompany', 'accomplish', 'accomplishing', 'according', 'accountability', 'accountant', 'accounting', 'accreditation', 'achieve', 'achievement', 'acting', 'action', 'active', 'activity', 'additional', 'addmission', 'address', 'adios', 'adjustment', 'administration', 'administrative', 'administrator', 'admission', 'admission-related', 'admitted', 'advanced', 'adviser', 'affair', 'affected', 'affidavit', 'affiliated', 'after', 'again', 'aim', 'all', 'alumnus', 'am', 'ambassador', 'amenity', 'among', 'an', 'and', 'announcement', 'another', 'any', 'anyone', 'apart', 'apparel', 'applicant', 'application', 'apply', 'applying', 'approach', 'architecture', 'are', 'area', 'arise', 'around', 'arrangement', 'arranging', 'art', 'article', 'aspiration', 'ass', 'assessing', 'assessment', 'assist', 'assistance', 'assistanceship', 'assistant', 'associated', 'association', 'assurance', 'at', 'athletic', 'attend', 'attending', 'audit', 'authenticated', 'authentication', 'auxiliary', 'available', 'availing', 'award', 'awarded', 'bachelor', 'background', 'bank', 'barangay', 'based', 'basic', 'batangas', 'bayan', 'be', 'been', 'before', 'begin', 'behind', 'being', 'benchmark', 'benefit', 'bid', 'bidding', 'billing', 'birth', 'board', 'body', 'book', 'borrow', 'breakdown', 'brief', 'bring', 'budget', 'built', 'bureau', 'buzz', 'by', 'bye', 'cafa', 'cafetaria', 'calculated', 'call', 'called', 'campus', 'can', 'canteen', 'cao', 'capability', 'capable', 'card', 'care', 'career', 'carlson', 'case', 'cashier', 'catalog', 'catch', 'cavite', 'celebrate', 'center', 'certificate', 'chairperson', 'change', 'charge', 'charity', 'chatbot', 'chatbots', 'chatting', 'check', 'checklist', 'ched', 'chemistry', 'chi', 'chief', 'choice', 'cie', 'cie.', 'cit', 'citizen', 'civil', 'cla', 'claim', 'class', 'close', 'closed', 'club', 'code', 'coe', 'collaboration', 'collect', 'college', 'come', 'commitment', 'committee', 'common', 'commonly', 'communication', 'community', 'company', 'compared', 'competence', 'competitive', 'completing', 'completion', 'compliance', 'compliant', 'component', 'composed', 'composition', 'computer', 'concept', 'concern', 'condition', 'conduct', 'conducted', 'confirm', 'confirmation', 'confirms', 'consequence', 'considering', 'constitutes', 'contact', 'contacted', 'contacting', 'context', 'continually', 'contribute', 'contribution', 'control', 'conversion', 'cooperating', 'coordinating', 'coordinator', 'core', 'cost', 'could', 'counseling', 'course', 'covered', 'create', 'created', 'creation', 'creator', 'credit', 'criterion', 'cuenca', 'cultural', 'curator', 'curious', 'current', 'curricular', 'curriculum', 'customer', 'dance', 'day', 'deadline', 'dean', 'decree', 'dedicated', 'define', 'degree', 'delivered', 'demonstrate', 'dental', 'department', 'department/college', 'dependent', 'deportation', 'describe', 'designated', 'designation', 'designed', 'detail', 'developed', 'developer', 'development', 'did', 'differ', 'different', 'digitally', 'dinner', 'director', 'disciplinary', 'discipline', 'discount', 'distribution', 'do', 'doctor', 'doctoral', 'document', 'documentation', 'doe', 'done', 'due', 'during', 'duty', 'each', 'early', 'easy', 'economic', 'economics', 'education', 'educational', 'educator', 'effect', 'efficiently', 'effort', 'elaborate', 'electrical', 'electronic', 'electronics', 'eligibility', 'email', 'embraced', 'emphasize', 'emphasizes', 'employ', 'employed', 'employment', 'empowerment', 'enforce', 'engineering', 'english', 'enhance', 'enhancement', 'enjoy', 'enlist', 'enlistment', 'enlistment/enrollment', 'enroll', 'enrollee', 'enrollement', 'enrollemnt', 'enrollment', 'enrollnment', 'ensure', 'entail', 'entrance', 'entrepreneurship', 'entry', 'equipment', 'equivalent', 'er', 'essay', 'established', 'establishment', 'estimated', 'eteeap', 'evaluation', 'event', 'evolve', 'evolved', 'exam', 'examination', 'example', 'excel', 'executive', 'exemplary', 'exist', 'existence', 'expanded', 'expect', 'expected', 'expense', 'expensive', 'explain', 'explaining', 'extension', 'external', 'extracurricular', 'facility', 'faculty', 'faculty-in-charge', 'farewell', 'fee', 'female', 'fifth', 'filipino', 'filled', 'final', 'finance', 'financial', 'find', 'fine', 'first', 'focal', 'focus', 'focusing', 'followed', 'food', 'for', 'foreign', 'form', 'format', 'formation', 'founding', 'fourth', 'freshman', 'from', 'fulfill', 'full', 'function', 'future', 'gator', 'gender', 'general', 'get', 'give', 'go', 'goal', 'good', 'goodbye', 'goodnight', 'governance', 'governing', 'gpa', 'grade', 'graduate', 'graduate-level', 'grant', 'granted', 'graphic', 'great', 'group', 'guidance', 'guide', 'guideline', 'guiding', 'guy', 'ha', 'halal', 'hand', 'handbook', 'handle', 'handled', 'handling', 'happening', 'happy', 'have', 'head', 'heard', 'held', 'hello', 'help', 'here', 'heyy', 'hi', 'high', 'higher', 'hiring', 'historical', 'history', 'hold', 'home', 'hospitality', 'hour', 'how', 'human', 'hybrid', 'hymn', 'i', 'ict', 'id', 'identification', 'if', 'immigration', 'important', 'improve', 'improvement', 'in', 'in-person', 'inception', 'included', 'incorporates', 'increased', 'individual', 'individually', 'industrial', 'industry', 'influenced', 'inform', 'information', 'infrastructure', 'initial', 'initially', 'initiative', 'innovation', 'inquiry', 'institution', 'institutional', 'institutional/international', 'instruction', 'instructional', 'instructor', 'insurance', 'integrated', 'interested', 'interesting', 'internal', 'into', 'introduce', 'involved', 'is', 'iskolar', 'issuance', 'issued', 'it', 'itself', 'job', 'join', 'joining', 'journey', 'k', 'kalinangan', 'keep', 'key', 'kind', 'know', 'kwento', 'l.', 'language', 'late', 'lately', 'later', 'latest', 'lead', 'leading', 'learn', 'learning', 'lecture', 'lecturer', 'led', 'letter', 'level', 'liberal', 'library', 'licensing', 'life', 'like', 'link', 'linkage', 'list', 'literacy', 'located', 'location', 'long', 'look', 'looking', 'lopez', 'lose', 'made', 'mail', 'main', 'maintenance', 'major', 'make', 'manage', 'management', 'manages', 'mandate', 'manila', 'many', 'mark', 'marykay', 'master', 'masteral', 'mathematics', 'matter', 'may', 'me', 'measure', 'mechanical', 'medical', 'medical/dental', 'meet', 'member', 'mentioned', 'milestone', 'mind', 'minimum', 'missed', 'mission', 'mode', 'modernize', 'more', 'morning', 'mt', 'much', 'museum', 'music', 'must', 'my', 'name', 'national', 'navigating', 'necessary', 'need', 'needed', 'network', 'new', 'news', 'next', 'ng', 'ngo', 'ni', 'nice', 'no', 'non-competitive', 'non-scholar', 'notarized', 'notation', 'notice', 'now', 'nstp', 'number', 'objective', 'obligation', 'obtain', 'obtaining', 'occurred', 'of', 'off', 'offer', 'offered', 'offering', 'office', 'office/innovation', 'office/internal', 'officer', 'official', 'oic-head', 'ok', 'okay', 'okie', 'okk', 'old', 'oldstudents', 'ols', 'on', 'one', 'online', 'open', 'opening', 'operate', 'operating', 'operation', 'opportunity', 'option', 'or', 'order', 'organization', 'organizational', 'origin', 'original', 'osa', 'out', 'outcome', 'outline', 'outlined', 'outlining', 'outlook', 'outside', 'over', 'overarching', 'overseeing', 'oversees', 'overview', 'page', 'paper', 'part', 'participate', 'pas', 'passed', 'passing', 'passport', 'past', 'path', 'pay', 'paying', 'pcat', 'peace', 'pending', 'people', 'percentage', 'perform', 'period', 'person', 'personnel', 'philippine', 'phone', 'photocopied', 'photocopy', 'physic', 'physical', 'picture', 'place', 'placement', 'plan', 'planning', 'plant', 'please', 'point', 'policy', 'portal', 'position', 'possible', 'poverty', 'power', 'pre-registration', 'premier', 'prepare', 'preparing', 'prerequisite', 'present', 'presentation', 'presented', 'president', 'presidential', 'previous', 'prez', 'principal', 'principle', 'printing', 'priority', 'privilege', 'problem', 'procedure', 'proceed', 'process', 'processing', 'procurement', 'productivity', 'professional', 'professor', 'proficiency', 'program', 'programmed', 'project', 'promote', 'property', 'prospect', 'prove', 'provide', 'provided', 'providing', 'provision', 'purpose', 'pursue', 'pwd', 'qualification', 'qualified', 'qualify', 'quality', 'query', 'quezon', 'ramos', 'rate', 'rating', 'reach', 'reached', 'read', 'reading', 'received', 'receiving', 'recent', 'recently', 'recipient', 'recognition', 'recommendation', 'record', 'records/scholastic', 'redirect', 'reduction', 'reference', 'reflected', 'regarding', 'regent', 'registrar', 'registration', 'regular', 'regulatory', 'related', 'relation', 'rendition', 'repair', 'report', 'required', 'requirement', 'research', 'reserve', 'resource', 'respective', 'responsibility', 'responsible', 'result', 'review', 'revision', 'right', 'role', 'rule', 'saturday', 'schedule', 'scheduled', 'scheduling', 'scholar', 'scholarship', 'school', 'science', 'scientific', 'seal', 'sealed', 'seating', 'second', 'secretary', 'section', 'sector', 'secure', 'securing', 'security', 'see', 'seek', 'semester', 'senior', 'serf', 'serve', 'service', 'serving', 'set', 'shaped', 'share', 'should', 'shs', 'significance', 'significant', 'signing', 'size', 'slip', 'smooth', 'so', 'social', 'society', 'some', 'something', 'soon', 'space', 'special', 'specialization', 'specializing', 'specific', 'specifically', 'specified', 'sport', 'staff', 'stakeholder', 'stand', 'standard', 'start', 'starting', 'state', 'stated', 'statement', 'status', 'statutory', 'stay', 'step', 'still', 'story', 'strategic', 'strategy', 'strength', 'strengthen', 'structure', 'student', 'study', 'subject', 'submission', 'submit', 'submitted', 'success', 'successful', 'such', 'summarize', 'supervision', 'supply', 'support', 'symbolize', 'system', 'taguig', 'take', 'taken', 'taking', 'target', 'task', 'teach', 'teacher', 'teaching', 'team', 'technical', 'technological', 'technology', 'telephone', 'tell', 'term', 'tertiary', 'test', 'text', 'thank', 'thanks', 'that', 'the', 'their', 'them', 'there', 'these', 'they', 'third', 'this', 'through', 'time', 'timeline', 'timing', 'title', 'to', 'top', 'touch', 'towards', 'trade', 'training', 'transcript', 'transfer', 'transferee', 'transferring', 'transition', 'transparency', 'troupe', 'tsu', 'tuition', 'tup', 'tup-manila', 'tupgpat', 'tupians', 'tutorial', 'type', 'typical', 'typically', 'u', 'under', 'undergrad', 'undergraduate', 'understand', 'undertake', 'undertaken', 'uniform', 'unit', 'unity', 'university', 'university/board', 'until', 'up', 'update', 'updated', 'upon', 'usual', 'usually', 'validated', 'value', 'vary', 'vegetarian', 'version', 'via', 'viability', 'view', 'violating', 'visa', 'visa/visa', 'visayas', 'vision', 'visit', 'vocational', 'vp', 'wa', 'want', 'way', 'we', 'weather', 'webpage', 'website', 'week', 'weekend', 'well', 'were', 'what', 'whats', 'whatsup', 'when', 'where', 'wheres', 'which', 'who', 'whom', 'why', 'with', 'within', 'work', 'working', 'wrote', 'ya', 'year', 'you', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Add to our words list\n",
    "        words.extend(w)\n",
    "        # Add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "\n",
    "words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [stemmer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # Output is a '0' for each tag and '1' for the current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(tokens, limit):\n",
    "    augmented_sentences = []\n",
    "    for i in range(len(tokens)):\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(tokens[i]):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) > 0:\n",
    "            num_augmentations = min(limit, len(synonyms))\n",
    "            sampled_synonyms = random.sample(synonyms, num_augmentations)\n",
    "            for synonym in sampled_synonyms:\n",
    "                augmented_tokens = tokens[:i] + [synonym] + tokens[i + 1:]\n",
    "                augmented_sentences.append(' '.join(augmented_tokens))\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the training data using synonym replacement\n",
    "augmented_data = []\n",
    "limit_per_tag = 100\n",
    "\n",
    "for i, doc in enumerate(training):\n",
    "    bag, output_row = doc\n",
    "    tokens = [words[j] for j in range(len(words)) if bag[j] == 1]\n",
    "    augmented_sentences = synonym_replacement(tokens, limit_per_tag)\n",
    "    for augmented_sentence in augmented_sentences:\n",
    "        augmented_bag = [1 if word in augmented_sentence else 0 for word in words]\n",
    "        augmented_data.append([augmented_bag, output_row])\n",
    "\n",
    "# print(augmented_data)\n",
    "# combined_data = np.concatenate((training, augmented_data), axis=0)\n",
    "# random.shuffle(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221233\n",
      "1942\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(augmented_data))\n",
    "print(len(training))\n",
    "\n",
    "print(type(augmented_data))\n",
    "print(type(training))\n",
    "\n",
    "combined_data = augmented_data + training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def separate_data_by_tags(data):\n",
    "    data_by_tags = {}\n",
    "    for d in data:\n",
    "        tag = tuple(d[1])\n",
    "        if tag not in data_by_tags:\n",
    "            data_by_tags[tag] = []\n",
    "        data_by_tags[tag].append(d)\n",
    "    return data_by_tags.values()\n",
    "\n",
    "\n",
    "separated_data = separate_data_by_tags(combined_data)\n",
    "\n",
    "# Lists to store training and testing data\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "# Split each tag's data into training and testing sets\n",
    "for tag_data in separated_data:\n",
    "    train_data, test_data = train_test_split(tag_data, test_size=0.2, random_state=42)\n",
    "    training_data.extend(train_data)\n",
    "    testing_data.extend(test_data)\n",
    "\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "# Convert training and testing data back to np.array\n",
    "train_x = np.array([d[0] for d in training_data])\n",
    "train_y = np.array([d[1] for d in training_data])\n",
    "test_x = np.array([d[0] for d in testing_data])\n",
    "test_y = np.array([d[1] for d in testing_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    true_labels = torch.argmax(targets, dim=1)\n",
    "    correct = (predicted_labels == true_labels).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_accuracy += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader.dataset)\n",
    "    average_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.0209, Training Accuracy: 0.5015\n",
      "Epoch [1/50], Testing Loss: 0.0090, Testing Accuracy: 0.8131\n",
      "Epoch [2/50], Training Loss: 0.0058, Training Accuracy: 0.8877\n",
      "Epoch [2/50], Testing Loss: 0.0039, Testing Accuracy: 0.9212\n",
      "Epoch [3/50], Training Loss: 0.0030, Training Accuracy: 0.9427\n",
      "Epoch [3/50], Testing Loss: 0.0024, Testing Accuracy: 0.9514\n",
      "Epoch [4/50], Training Loss: 0.0020, Training Accuracy: 0.9594\n",
      "Epoch [4/50], Testing Loss: 0.0018, Testing Accuracy: 0.9629\n",
      "Epoch [5/50], Training Loss: 0.0015, Training Accuracy: 0.9680\n",
      "Epoch [5/50], Testing Loss: 0.0015, Testing Accuracy: 0.9677\n",
      "Epoch [6/50], Training Loss: 0.0013, Training Accuracy: 0.9735\n",
      "Epoch [6/50], Testing Loss: 0.0013, Testing Accuracy: 0.9727\n",
      "Epoch [7/50], Training Loss: 0.0011, Training Accuracy: 0.9769\n",
      "Epoch [7/50], Testing Loss: 0.0012, Testing Accuracy: 0.9751\n",
      "Epoch [8/50], Training Loss: 0.0010, Training Accuracy: 0.9790\n",
      "Epoch [8/50], Testing Loss: 0.0011, Testing Accuracy: 0.9781\n",
      "Epoch [9/50], Training Loss: 0.0009, Training Accuracy: 0.9809\n",
      "Epoch [9/50], Testing Loss: 0.0010, Testing Accuracy: 0.9792\n",
      "Epoch [10/50], Training Loss: 0.0008, Training Accuracy: 0.9823\n",
      "Epoch [10/50], Testing Loss: 0.0009, Testing Accuracy: 0.9809\n",
      "Epoch [11/50], Training Loss: 0.0008, Training Accuracy: 0.9836\n",
      "Epoch [11/50], Testing Loss: 0.0009, Testing Accuracy: 0.9815\n",
      "Epoch [12/50], Training Loss: 0.0007, Training Accuracy: 0.9845\n",
      "Epoch [12/50], Testing Loss: 0.0009, Testing Accuracy: 0.9818\n",
      "Epoch [13/50], Training Loss: 0.0007, Training Accuracy: 0.9852\n",
      "Epoch [13/50], Testing Loss: 0.0008, Testing Accuracy: 0.9834\n",
      "Epoch [14/50], Training Loss: 0.0007, Training Accuracy: 0.9854\n",
      "Epoch [14/50], Testing Loss: 0.0008, Testing Accuracy: 0.9834\n",
      "Epoch [15/50], Training Loss: 0.0006, Training Accuracy: 0.9864\n",
      "Epoch [15/50], Testing Loss: 0.0008, Testing Accuracy: 0.9839\n",
      "Epoch [16/50], Training Loss: 0.0006, Training Accuracy: 0.9868\n",
      "Epoch [16/50], Testing Loss: 0.0007, Testing Accuracy: 0.9844\n",
      "Epoch [17/50], Training Loss: 0.0006, Training Accuracy: 0.9871\n",
      "Epoch [17/50], Testing Loss: 0.0007, Testing Accuracy: 0.9835\n",
      "Epoch [18/50], Training Loss: 0.0006, Training Accuracy: 0.9874\n",
      "Epoch [18/50], Testing Loss: 0.0007, Testing Accuracy: 0.9853\n",
      "Epoch [19/50], Training Loss: 0.0006, Training Accuracy: 0.9879\n",
      "Epoch [19/50], Testing Loss: 0.0007, Testing Accuracy: 0.9848\n",
      "Epoch [20/50], Training Loss: 0.0005, Training Accuracy: 0.9879\n",
      "Epoch [20/50], Testing Loss: 0.0007, Testing Accuracy: 0.9853\n",
      "Epoch [21/50], Training Loss: 0.0005, Training Accuracy: 0.9885\n",
      "Epoch [21/50], Testing Loss: 0.0007, Testing Accuracy: 0.9859\n",
      "Epoch [22/50], Training Loss: 0.0005, Training Accuracy: 0.9885\n",
      "Epoch [22/50], Testing Loss: 0.0007, Testing Accuracy: 0.9851\n",
      "Epoch [23/50], Training Loss: 0.0005, Training Accuracy: 0.9889\n",
      "Epoch [23/50], Testing Loss: 0.0007, Testing Accuracy: 0.9861\n",
      "Epoch [24/50], Training Loss: 0.0005, Training Accuracy: 0.9886\n",
      "Epoch [24/50], Testing Loss: 0.0007, Testing Accuracy: 0.9842\n",
      "Epoch [25/50], Training Loss: 0.0005, Training Accuracy: 0.9890\n",
      "Epoch [25/50], Testing Loss: 0.0007, Testing Accuracy: 0.9855\n",
      "Epoch [26/50], Training Loss: 0.0005, Training Accuracy: 0.9889\n",
      "Epoch [26/50], Testing Loss: 0.0006, Testing Accuracy: 0.9866\n",
      "Epoch [27/50], Training Loss: 0.0005, Training Accuracy: 0.9895\n",
      "Epoch [27/50], Testing Loss: 0.0007, Testing Accuracy: 0.9856\n",
      "Epoch [28/50], Training Loss: 0.0005, Training Accuracy: 0.9896\n",
      "Epoch [28/50], Testing Loss: 0.0006, Testing Accuracy: 0.9867\n",
      "Epoch [29/50], Training Loss: 0.0005, Training Accuracy: 0.9895\n",
      "Epoch [29/50], Testing Loss: 0.0006, Testing Accuracy: 0.9863\n",
      "Epoch [30/50], Training Loss: 0.0005, Training Accuracy: 0.9899\n",
      "Epoch [30/50], Testing Loss: 0.0006, Testing Accuracy: 0.9865\n",
      "Epoch [31/50], Training Loss: 0.0004, Training Accuracy: 0.9900\n",
      "Epoch [31/50], Testing Loss: 0.0007, Testing Accuracy: 0.9863\n",
      "Epoch [32/50], Training Loss: 0.0004, Training Accuracy: 0.9899\n",
      "Epoch [32/50], Testing Loss: 0.0006, Testing Accuracy: 0.9876\n",
      "Epoch [33/50], Training Loss: 0.0004, Training Accuracy: 0.9905\n",
      "Epoch [33/50], Testing Loss: 0.0006, Testing Accuracy: 0.9868\n",
      "Epoch [34/50], Training Loss: 0.0004, Training Accuracy: 0.9900\n",
      "Epoch [34/50], Testing Loss: 0.0006, Testing Accuracy: 0.9872\n",
      "Epoch [35/50], Training Loss: 0.0004, Training Accuracy: 0.9902\n",
      "Epoch [35/50], Testing Loss: 0.0006, Testing Accuracy: 0.9872\n",
      "Epoch [36/50], Training Loss: 0.0004, Training Accuracy: 0.9907\n",
      "Epoch [36/50], Testing Loss: 0.0006, Testing Accuracy: 0.9869\n",
      "Epoch [37/50], Training Loss: 0.0004, Training Accuracy: 0.9906\n",
      "Epoch [37/50], Testing Loss: 0.0006, Testing Accuracy: 0.9876\n",
      "Epoch [38/50], Training Loss: 0.0004, Training Accuracy: 0.9907\n",
      "Epoch [38/50], Testing Loss: 0.0006, Testing Accuracy: 0.9868\n",
      "Epoch [39/50], Training Loss: 0.0004, Training Accuracy: 0.9905\n",
      "Epoch [39/50], Testing Loss: 0.0006, Testing Accuracy: 0.9876\n",
      "Epoch [40/50], Training Loss: 0.0004, Training Accuracy: 0.9906\n",
      "Epoch [40/50], Testing Loss: 0.0006, Testing Accuracy: 0.9869\n",
      "Epoch [41/50], Training Loss: 0.0004, Training Accuracy: 0.9909\n",
      "Epoch [41/50], Testing Loss: 0.0006, Testing Accuracy: 0.9872\n",
      "Epoch [42/50], Training Loss: 0.0004, Training Accuracy: 0.9908\n",
      "Epoch [42/50], Testing Loss: 0.0006, Testing Accuracy: 0.9869\n",
      "Epoch [43/50], Training Loss: 0.0004, Training Accuracy: 0.9910\n",
      "Epoch [43/50], Testing Loss: 0.0006, Testing Accuracy: 0.9873\n",
      "Epoch [44/50], Training Loss: 0.0004, Training Accuracy: 0.9910\n",
      "Epoch [44/50], Testing Loss: 0.0006, Testing Accuracy: 0.9867\n",
      "Epoch [45/50], Training Loss: 0.0004, Training Accuracy: 0.9910\n",
      "Epoch [45/50], Testing Loss: 0.0006, Testing Accuracy: 0.9874\n",
      "Epoch [46/50], Training Loss: 0.0004, Training Accuracy: 0.9913\n",
      "Epoch [46/50], Testing Loss: 0.0006, Testing Accuracy: 0.9873\n",
      "Epoch [47/50], Training Loss: 0.0004, Training Accuracy: 0.9909\n",
      "Epoch [47/50], Testing Loss: 0.0006, Testing Accuracy: 0.9874\n",
      "Epoch [48/50], Training Loss: 0.0004, Training Accuracy: 0.9913\n",
      "Epoch [48/50], Testing Loss: 0.0006, Testing Accuracy: 0.9878\n",
      "Epoch [49/50], Training Loss: 0.0004, Training Accuracy: 0.9913\n",
      "Epoch [49/50], Testing Loss: 0.0006, Testing Accuracy: 0.9872\n",
      "Epoch [50/50], Training Loss: 0.0004, Training Accuracy: 0.9914\n",
      "Epoch [50/50], Testing Loss: 0.0006, Testing Accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and testing data\n",
    "train_x = torch.tensor(train_x).float()\n",
    "train_y = torch.tensor(train_y).float()\n",
    "test_x = torch.tensor(test_x).float()\n",
    "test_y = torch.tensor(test_y).float()\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "test_dataset = CustomDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = len(train_x[0])\n",
    "hidden_size = 8\n",
    "output_size = len(train_y[0])\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model and evaluate on the testing set\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_acc += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_acc / len(train_loader.dataset)\n",
    "\n",
    "    # Print training loss and accuracy for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on the testing set\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, input_size, hidden_size, output_size):\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the input sentence\n",
    "def preprocess_sentence(sentence, words):\n",
    "    sentence_words = sentence.lower().split()\n",
    "    sentence_words = [word for word in sentence_words if word in words]\n",
    "    return sentence_words\n",
    "\n",
    "# Function to convert the preprocessed sentence into a feature vector\n",
    "def sentence_to_features(sentence_words, words):\n",
    "    features = [1 if word in sentence_words else 0 for word in words]\n",
    "    return torch.tensor(features).float().unsqueeze(0)\n",
    "\n",
    "# Function to generate a response using the trained model\n",
    "def generate_response(sentence, model, words, classes):\n",
    "    sentence_words = preprocess_sentence(sentence, words)\n",
    "    if len(sentence_words) == 0:\n",
    "        return \"I'm sorry, but I don't understand. Can you please rephrase or provide more information?\"\n",
    "\n",
    "    features = sentence_to_features(sentence_words, words)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "\n",
    "    probabilities, predicted_class = torch.max(outputs, dim=1)\n",
    "    confidence = probabilities.item()\n",
    "    predicted_tag = classes[predicted_class.item()]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "        for intent in intents['intents']:\n",
    "            if intent['tag'] == predicted_tag:\n",
    "                return random.choice(intent['responses'])\n",
    "\n",
    "    return \"I'm sorry, but I'm not sure how to respond to that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.\n",
      "Hello\n",
      "Good to see you! How can I assist you?\n",
      "----------------------------------------------------------------\n",
      "who are you?\n",
      "My name's TekBot, and I'm here to answer your curiosity about Technological University of the Philippines (TUP)!\n",
      "----------------------------------------------------------------\n",
      "what is the vision, mission, and core values of computer science?\n",
      "The current principal of TUP is Engr. Reynaldo P. Ramos, PhD., EnP.\n",
      "----------------------------------------------------------------\n",
      "is there an admission fee and how much?\n",
      "I'm sorry, but I'm not sure how to respond to that.\n",
      "----------------------------------------------------------------\n",
      "what time is the office of university open?\n",
      "Sure! Here are the links to TUP Manila's online services: \n",
      "\n",
      "Student Applicant ERS: [Student Applicant ERS](https://ers.tup.edu.ph/aims/applicants/) \n",
      "Student ERS: [Student ERS](https://ers.tup.edu.ph/aims/students/) \n",
      "Faculty ERS: [Faculty ERS](https://ers.tup.edu.ph/aims/faculty/) \n",
      "Bids Opportunity: [Bids Opportunity](https://tup.edu.ph/procurement) \n",
      "Courses Offered: [Courses Offered](https://tup.edu.ph/undergraduate/admission/undergraduate-programs) \n",
      "Transparency Seal: [Transparency Seal](https://tup.edu.ph/transparency-seal)\n",
      "----------------------------------------------------------------\n",
      "are they offering scholarships?\n",
      "Develop, improve and implement training program to attain effective delivery system of technological education.\n",
      "Strengthen faculty qualification and rank.\n",
      "Provide opportunities for faculty researches and come up with new concepts, materials and processes.\n",
      "Make available the facilities of the College and expertise of the faculty to the community.\n",
      "----------------------------------------------------------------\n",
      "what are the program offer?\n",
      "Sure! Here's how to apply for the TUP Graduate Programs:\n",
      "\n",
      "ADMISSION PROCEDURE\n",
      "1. Submit the following requirements in a long brown envelope. Only complete and accomplished requirements will be processed.\n",
      " Accomplished Application for Admission Form (TUP Graduate Program)\n",
      "The form can be downloaded at the TUP website\n",
      "An essay of not less than 100 words stating the applicant's reason for applying in the program. (To be written/typewritten at the back of the Application for Admission Form)\n",
      "Certified True Copy of the Transcript of Records\n",
      " Two (2) sealed recommendation letters The recommendation letter should describe the applicant's qualifications and potential for successfully completing graduate studies at TUP. It may also include comments and relevant insights such as on the applicant's character and outstanding skills/talents in line with the program applied for.\n",
      " Three (3) passport size pictures with name tag on white background\n",
      " Certificate of English proficiency as second language (for foreign student applicants only)\n",
      " Application requirements should be submitted to the:\n",
      "\n",
      "Director, Graduate Program \n",
      "Technology University of the Philippines \n",
      "Ayala Blvd., Ermita, Manila, 1000 Philippines \n",
      "Telephone No. (02)301-3001 loc. 704\n",
      "\n",
      "2. After passing the initial evaluation of application requirements, secure and accomplish the TUP Graduate Program Admission Test (TUPGPAT) Application Form at the Office of Graduate Program, 2/F CAP Building.\n",
      "3. Present the TUPGPAT Application Form to the cashier's office for payment.\n",
      "Master's Program - P900.00\n",
      "Doctoral Program - P1,050.00\n",
      "4. Proceed to the Guidance Office, G/F CLA Building, and present the Official Receipt (O.R.) with the TUPGPAT Application Form for scheduling.\n",
      "5. On the scheduled date and time of examination indicated in the form, present the Official Receipt (O.R.) with the TUPGPAT Application Form to the administering officer/proctor at the Guidance Office, G/F CLA Building.\n",
      "----------------------------------------------------------------\n",
      "what is the mode of classes?\n",
      "I'm here to help you with a wide range of tasks about TUP such as answering questions, providing information, and offering assistance. Just let me know what you need!\n",
      "----------------------------------------------------------------\n",
      "is it face-to-face or online?\n",
      "There is a huge and spacious library available at TUP. The library operates from 8:00 AM to 6:00 PM. For more information, you can visit the TUP Library Facebook page: https://www.facebook.com/tuplibrary\n",
      "----------------------------------------------------------------\n",
      "does the college accept transferee or late enrolee?\n",
      "Graduates of this program typically have diverse career opportunities available to them. They can explore roles in various industries related on their course, or pursue further education and professional development. The skills acquired during the program prepare them for a wide range of career paths, offering flexibility and potential for growth.\n",
      "----------------------------------------------------------------\n",
      "when are the class starts and when it usually ends\n",
      "TUP provides various amenities such as auditoriums, a library and information center, an indoor court, a gymnasium, accommodation facilities, a dining area, labs, a medical clinic, a chapel, diverse clubs, a performance stage, and an open field.\n",
      "----------------------------------------------------------------\n",
      "is there a supplementary admission requirements?\n",
      "Sure! Here are the Admission Procedure of Technological University of the Philippines-Manila:\n",
      "\n",
      "ATTENTION:Before going into the admissions process, you must first set up an ERS (Electronic Registration System) account. By registering on the ERS platform, applicants can shorten their application process, resulting in better communication and document processing. This phase not only streamlines the application process, but also allows the institution to provide targeted support and guidance to potential students. Visit https://ers.tup.edu.ph/aims/applicants/ to set up your ERS account and begin the admissions process.\n",
      "\n",
      "Pdf: https://drive.google.com/file/d/1lYm4BfKaB711UKwi7a41xuw9DYVXoPtR/view \n",
      "\n",
      "ADMISSION PROCEDURE\n",
      "QUALIFIED APPLICANTS\n",
      "ACADEMIC YEAR 2023-2024\n",
      "STEP 1: REPORT TO OFFICE OF ADMISSIONS\n",
      "PRESENT THE FOLLOWING DOCUMENTS\n",
      "a. Pre-Registration Form (Printed and Filled Out)\n",
      "b. Applicant Patient Record Form (Printed and Filled Out)\n",
      "c. Original & Photocopy of Grade 12 SHS Card\n",
      "d. Certificate of Good Moral Character\n",
      "e. PSA/NSO Birth Certificate (Original Photocopy) & PSA/NSO Marriage Certificate (Original & Photocopy for married only) female students only)\n",
      "STEP 2: REPORT TO COLLEGE FOR INTERVIEW\n",
      "STEP 3: REPORT TO MEDICAL/DENTAL CLINIC MEDICAL REQUIREMENTS (BRING ORIGINAL & PHOTOCOPY) TEC\n",
      "PRESENT THE FOLLOWING DOCUMENTS\n",
      "a. Chest X-Ray (PA View)\n",
      "   b. Drug Test\n",
      "STEP 4: REPORT TO OFFICE OF ADMISSIONS\n",
      "REPORT BACK TO OFFICE OF ADMISSIONS FOR NOTICE OF ADMISSION, UNDERTAKING AND STUDENT PLEDGE\n",
      "STEP 5: REPORT TO COLLEGE FOR ENLISTMENT\n",
      "STEP 6: CONFIRMATION OF ENROLLMENT BY THE REGISTRAR\n",
      "----------------------------------------------------------------\n",
      "what is the career outlook for graduates?\n",
      "Graduates of this program typically have diverse career opportunities available to them. They can explore roles in various industries related on their course, or pursue further education and professional development. The skills acquired during the program prepare them for a wide range of career paths, offering flexibility and potential for growth.\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model.pth'\n",
    "input_size = len(words)\n",
    "hidden_size = 8\n",
    "output_size = len(classes)\n",
    "model = load_model(model_path, input_size, hidden_size, output_size)\n",
    "\n",
    "# Test the chatbot response\n",
    "print('Hello! I am a chatbot. How can I help you today? Type \"quit\" to exit.')\n",
    "while True:\n",
    "    user_input = input('> ')\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    print(user_input)\n",
    "    response = generate_response(user_input, model, words, classes)\n",
    "    print(response)\n",
    "    print('----------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
